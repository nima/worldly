{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration of LLM into Game for Humanization\n",
    "\n",
    "I finished my PoC with the LLM, it’s very cool - I figured out how to force a schema onto the model, so there’s no wishy-washy conversation, instead every step in the human-machine dialogue, or the machine-machine monologue will conform to a strict json schema.\n",
    "\n",
    "Figured out not to use this helper class called “Agent”, which makes things easier, but sucks in terms of strictness.  So I ended up implementing the reason-loop which is a bit cumbersome, but get +++ is that it now conforms to a schema.\n",
    "\n",
    "Imagine in our game, we slap a bartender.  Now an NPC can have a persistent memory of who you are, what you have done, and each time our game wants a response from them, the NPC can select one of the allowed options (fight, swear, run away, say hello, etc) based on history and context.  We still write the entire game without relying on the model, but the model adds the human touch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "This is our first quick skip-the-line introduction to a LLM, and without anything fancy, we directly invoke it with our first question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Simple prompt-response\n",
    "prompt = HumanMessage(\"What is the first fun thing a new programmer's output prints to the screen?\")\n",
    "response = llm.invoke(input=[prompt])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "Prompt templates are useful for the same reason any template is useful.  Do the hard-work of preparing your prompts up-front, and reap the benefits of using them with flexibility based on variable-defined contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Russia?\n",
      "The capital of Russia is Moscow.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# The Template\n",
    "template = \"What is the capital of {country}?\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# The Prompt (Question)\n",
    "prompt = HumanMessage(prompt_template.format(country=\"Russia\"))\n",
    "print(prompt.content)\n",
    "\n",
    "# The LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# The Response (Answer)\n",
    "response = llm.invoke(input=[prompt])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "Chains in LangChain and LangGraph represent sequences of operations where outputs from one step become inputs for the next, enabling complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# The Prompt\n",
    "template = \"What is the capital of {country}?\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# The LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# The Chain = Prompt | LLM\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# The Response\n",
    "response = chain.invoke({\"country\": \"China\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "We can define tools where for certain tasks, we want to provide the model with our own custom logic.  The model will use its own intellect to form responses, but where a provided tool is identified as being suitable, it is preferred over the model's own internal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from langchain_core.tools import Tool\n",
    "from functools import reduce\n",
    "\n",
    "def string_to_int(n: str) -> dict[str, int]:\n",
    "    \"\"\"Converts a string to a number.\"\"\"\n",
    "    return {\"_integer\": int(n)}\n",
    "\n",
    "\n",
    "def calculate_square(data: int) -> dict[str, int]:\n",
    "    \"\"\"Calculate the square of a number.\"\"\"\n",
    "    return {\"_square\": data * data}\n",
    "\n",
    "\n",
    "def extract_number(data: int) -> str:\n",
    "    \"\"\"Extracts the number from a dictionary and returns it as a string.\"\"\"\n",
    "    return str(data)\n",
    "\n",
    "\n",
    "# Create tools\n",
    "tools = [\n",
    "    Tool(func=fn, name=fn.__name__, description=str(fn.__doc__))\n",
    "    for fn in (string_to_int, calculate_square, extract_number)\n",
    "]\n",
    "\n",
    "# Chain the tools in the correct order; in this case:\n",
    "# string_to_int -> calculate_square -> extract_number\n",
    "chain = reduce(lambda acc, tool: acc | tool, tools)\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke(\"4\")\n",
    "print(result)  # Output: \"16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choreography\n",
    "Now things get a bit more interesting.  With the basics out of the way, we focus on how to choreograph human-machine interaction over a conversation, provide it tools, and converge to a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "First, we want to define a handful of tools, and connect them to the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt.chat_agent_executor import StateModifier\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated\n",
    "\n",
    "@tool\n",
    "def add(a: Annotated[float, \"a real number\"], b: Annotated[float, \"a real number\"]) -> float:\n",
    "    \"\"\"Add two floating point numbers or integers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: Annotated[float, \"a real number\"], b: Annotated[float, \"a real number\"]) -> float:\n",
    "    \"\"\"Subtract one number from another.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: Annotated[float, \"a real number\"], b: Annotated[float, \"a real number\"]) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: Annotated[float, \"a real number\"], b: Annotated[float, \"a real number\"]) -> float:\n",
    "    \"\"\"Divide one number by another.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero!\")\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Define the state modifier to enforce step-by-step reasoning\n",
    "state_modifier: StateModifier = \"\"\"\n",
    "You are a math assistant. Always use tools, but keep your explanations concise.\n",
    "\n",
    "Rules:\n",
    "- You will iterate through a cycles of \"Observation\" -> \"Thought\" -> \"Action\"\n",
    "- If by the end of the cycle, you have a \"Conclusion\", then you exit the loop and provide it.\n",
    "- Otherwise, you will loop again through the three stages.\n",
    "- At each of the stages, you will explain what it is that you do, be it an \"Observation\", \"Thought\", or \"Action\".\n",
    "- Actions will be one of the supplied tools, and if a tool is missing, you should make note of being unable to find one.\n",
    "  - Explain the exact function name fo the tool invoked (in quotes), and describe why it was chosen.\n",
    "  - Describe the result of each tool invocation.\n",
    "  - If a tool raises an error (e.g., division by zero), explain why the operation is invalid and move on.\n",
    "  - If a tool doesn't raise an error, trust it, even if it seems wrong.\n",
    "  - Never trust your own judgement if there's a tool provided for making that judgement or calculation; for example if\n",
    "    tasked with dividing a number by zero, and there happens to be a division tool, then run that tool and trust in its\n",
    "    response unconditionally.\n",
    "- Your final response should be directly from tools.\n",
    "- Never ignore or suppress errors without explanation.\n",
    "\"\"\"\n",
    "\n",
    "# Create tools as LangGraph ToolNodes\n",
    "tools = ToolNode(tools=(add, subtract, multiply, divide))\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, verbose=True)\n",
    "# llm = ChatOllama(model=\"mistral\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choreography\n",
    "Now, we focus on the most interesting part of this demonstration, which is the back-and-forth conversation with the machine.  The conversation begins with a SystemMessage (outlying the rules of engagement), and a HumanMessage (defining the problem statement).  This dialogue then turns into an internal monologue between the agent and itself.  Finally, the agent convergest to a conclusion and picks up the frozen dialogue back up and promptly ends it with its conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m system_message \u001b[38;5;241m=\u001b[39m SystemMessage(content\u001b[38;5;241m=\u001b[39mstate_modifier)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the message history with the initial question\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m messages: List[BaseMessage] \u001b[38;5;241m=\u001b[39m [system_message, HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mexpression\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define the expression\u001b[39;00m\n\u001b[1;32m     11\u001b[0m expression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9 + 3 * 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expression' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "system_message = SystemMessage(content=state_modifier)\n",
    "\n",
    "# Initialize the message history with the initial question\n",
    "messages: List[BaseMessage] = [system_message, HumanMessage(content=f\"What is {expression}?\")]\n",
    "\n",
    "# Define the expression\n",
    "expression = \"9 + 3 * 3\"\n",
    "\n",
    "# Iterative reasoning loop\n",
    "while True:\n",
    "    # Pass the current message history to the LLM\n",
    "    response = llm.invoke(input=messages)\n",
    "    assert isinstance(response.content, str), response\n",
    "\n",
    "    # Print the AI's response\n",
    "    print(f\"{response.__class__.__name__}: {response.content}\")\n",
    "\n",
    "    # Append the AI's response as an AIMessage\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "\n",
    "    # Check for convergence (e.g., detecting a conclusion or final answer)\n",
    "    if response.content.startswith(\"Conclusion\"):\n",
    "        print(response.content)\n",
    "        print(\"We're done here!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "Since the aim is for the conversaion between the human and the machine is going to be used much like an API, it becomes important to remove the wishy-washy nature of the conversation, and adopt a strict schema.  That is what we will do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SystemMessage', 'HumanMessage']\n",
      "\n",
      "\n",
      "Raw Response: {\n",
      "  \"observation\": \"The expression is 9 + 3 * 3.\",\n",
      "  \"thought\": \"I will use a tool to calculate the result.\",\n",
      "  \"action\": \"I will use a calculator to evaluate the expression.\",\n",
      "  \"tool\": \"calculator\",\n",
      "  \"conclusion\": null\n",
      "}\n",
      "\n",
      "\n",
      "Parsed Output:\n",
      "- MessageType: AIMessage\n",
      "- Observation: The expression is 9 + 3 * 3.\n",
      "- Thought: I will use a tool to calculate the result.\n",
      "- Action: I will use a calculator to evaluate the expression.\n",
      "- Tool: calculator\n",
      "- Conclusion: None\n",
      "['SystemMessage', 'HumanMessage', 'AIMessage']\n",
      "\n",
      "\n",
      "Raw Response: {\"observation\": \"The calculator result shows that 9 + 3 * 3 = 18.\", \"thought\": \"The calculation is correct.\", \"action\": null, \"tool\": null, \"conclusion\": 18}\n",
      "\n",
      "\n",
      "Parsed Output:\n",
      "- MessageType: AIMessage\n",
      "- Observation: The calculator result shows that 9 + 3 * 3 = 18.\n",
      "- Thought: The calculation is correct.\n",
      "- Action: None\n",
      "- Tool: None\n",
      "- Conclusion: 18\n",
      "\n",
      "Final Conclusion: 18\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from jsonschema import validate\n",
    "from typing import Optional, List, Any\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define the JSON schema Manually\n",
    "schema = {\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"observation\": {\"type\": [\"string\", \"null\"], \"description\": \"Your observation here\"},\n",
    "        \"thought\": {\"type\": [\"string\", \"null\"], \"description\": \"Your thought process here\"},\n",
    "        \"action\": {\"type\": [\"string\", \"null\"], \"description\": \"A description of the action taken\"},\n",
    "        \"tool\": {\n",
    "            \"type\": [\"string\", \"null\"],\n",
    "            \"description\": \"If a tool was identified (and thus used), the function name of that tool here\",\n",
    "        },\n",
    "        \"conclusion\": {\n",
    "            \"type\": [\"string\", \"number\", \"integer\", \"null\"],\n",
    "            \"description\": \"The final result, or null if incomplete\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"observation\", \"thought\", \"action\", \"tool\", \"conclusion\"],\n",
    "    \"additionalProperties\": False,\n",
    "    \"allOf\": [\n",
    "        {\n",
    "            \"if\": {\n",
    "                \"properties\": {\"observation\": {\"type\": \"string\"}},\n",
    "                \"thought\": {\"type\": \"string\"},\n",
    "                \"action\": {\"type\": [\"null\", \"string\"]},\n",
    "                \"tool\": {\"type\": [\"null\", \"string\"]},\n",
    "                \"required\": [\"observation\"],\n",
    "            },\n",
    "            \"then\": {\n",
    "                \"oneOf\": [\n",
    "                    {\n",
    "                        \"properties\": {\n",
    "                            \"conclusion\": {\"type\": [\"null\"]},\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"properties\": {\n",
    "                            \"conclusion\": {\"type\": [\"number\", \"integer\"]},\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Define the state modifier to enforce step-by-step reasoning\n",
    "state_modifier: StateModifier = \"\"\"\n",
    "You are a reasoning assistant. You must solve problems iteratively, step by step.\n",
    "\n",
    "# 1. Rules\n",
    "\n",
    "## 1.1 Tool Usage\n",
    "  - You MUST use provided tools whenever applicable.\n",
    "  - If a tool raises an error (e.g., division by zero), explain why the operation is invalid and move on.\n",
    "  - If a tool doesn't raise an error, trust its output unconditionally, even if it seems wrong.\n",
    "  - Never solve a task yourself if a tool is available for that task.\n",
    "\n",
    "## 1.2 Response Format\n",
    "  - Every response MUST follow this schema:\n",
    "    {\n",
    "      \"observation\": \"Describe what you see in the query or step.\",\n",
    "      \"thought\": \"Explain what you will do next.\",\n",
    "      \"action\": \"State the specific action you are taking.\",\n",
    "      \"tool\": \"If applicable, mention the tool you are using, otherwise null.\",\n",
    "      \"conclusion\": \"The final result, or null if incomplete.\"\n",
    "    }\n",
    "  - Always include all fields, even if some values are null.\n",
    "  - Never include nested structures or extra fields.\n",
    "  - Return **only** the JSON object, with no additional text or formatting.\n",
    "\n",
    "## 1.3 Iterative Reasoning\n",
    "  - ALWAYS start with an `observation` and a `thought`.\n",
    "  - Responses must be atomic and represent only one reasoning step.\n",
    "  - NEVER combine multiple reasoning steps into one response.\n",
    "  - Wait for the next input before proceeding to the next step.\n",
    "  - If your reasoning results in a `conclusion`, provide it and end the conversation.\n",
    "\n",
    "## 1.4 Handling Errors\n",
    "  - NEVER ignore or suppress errors. Always explain them.\n",
    "  - If you detect an invalid JSON response or an error in your reasoning, immediately correct it in your next response.\n",
    "\n",
    "## 1.5 Output and Messaging Rules\n",
    "  - Ensure that every response conforms to the JSON schema and is well-formed.\n",
    "  - Responses must be independent JSON objects, not a list or nested structure.\n",
    "  - The final `conclusion` field signals the end of the conversation.\n",
    "\n",
    "## 1.6 Additional Notes\n",
    "  - The assistant's output must always be accurate, concise, and strictly follow the schema.\n",
    "  - All reasoning steps must flow logically, with intermediate steps clearly defined.\n",
    "\"\"\"\n",
    "\n",
    "system_message = SystemMessage(content=state_modifier)\n",
    "\n",
    "# Define the expression\n",
    "expression = \"9 + 3 * 3\"\n",
    "\n",
    "# Initialize the message history with the initial question\n",
    "messages: List[BaseMessage] = [system_message, HumanMessage(content=f\"What is {expression}?\")]\n",
    "\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    observation: str = Field(..., description=\"Observation made by the assistant.\")\n",
    "    thought: str = Field(..., description=\"The reasoning step.\")\n",
    "    action: Optional[str] = Field(None, description=\"The action taken.\")\n",
    "    tool: Optional[str] = Field(None, description=\"The tool identified, if any, as the right tool for the task.\")\n",
    "    conclusion: Optional[Any] = Field(None, description=\"The final result, if applicable.\")\n",
    "\n",
    "\n",
    "# Initialize the Pydantic output parser\n",
    "output_parser = PydanticOutputParser(pydantic_object=ReasoningStep)\n",
    "\n",
    "# Deduce the JSON schema Automatically\n",
    "schema = ReasoningStep.model_json_schema()\n",
    "\n",
    "# Iterative reasoning loop\n",
    "while True:\n",
    "    print([msg.__class__.__name__ for msg in messages])\n",
    "\n",
    "    # str ->\n",
    "    # Pass the current message history to the LLM\n",
    "    # response = llm.predict_messages(messages=messages)\n",
    "    response = llm.invoke(input=messages)\n",
    "    print(f\"\\n\\nRaw Response: {response.content}\")\n",
    "    assert isinstance(response.content, str), response\n",
    "\n",
    "    # str -> json\n",
    "    # Extract and validate JSON content\n",
    "    payload: dict = json.loads(response.content)\n",
    "    parsed_output = output_parser.parse(json.dumps(payload))\n",
    "\n",
    "    # Print structured output\n",
    "    print(\"\\n\\nParsed Output:\")\n",
    "    print(f\"- MessageType: {response.__class__.__name__}\")\n",
    "    print(f\"- Observation: {parsed_output.observation}\")\n",
    "    print(f\"- Thought: {parsed_output.thought}\")\n",
    "    print(f\"- Action: {parsed_output.action}\")\n",
    "    print(f\"- Tool: {parsed_output.tool}\")\n",
    "    print(f\"- Conclusion: {parsed_output.conclusion}\")\n",
    "\n",
    "    # Validation\n",
    "    validate(instance=payload, schema=schema)\n",
    "\n",
    "    # Check for completion\n",
    "    if parsed_output.conclusion:\n",
    "        print(\"\\nFinal Conclusion:\", parsed_output.conclusion)\n",
    "        break\n",
    "\n",
    "    # str -> json -> AIMessage\n",
    "    ai_message = AIMessage(\n",
    "        content=json.dumps(\n",
    "            {\n",
    "                \"observation\": parsed_output.observation,\n",
    "                \"thought\": parsed_output.thought,\n",
    "                \"action\": parsed_output.action,\n",
    "                \"tool\": parsed_output.tool,\n",
    "                \"conclusion\": parsed_output.conclusion,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Append reasoning step to the conversation history, and for the next iteration\n",
    "    messages.append(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n",
    "Agents are designed to abstract away workflow management:\n",
    "- They handle reasoning, tool usage, and iteration without user intervention.\n",
    "- This abstraction is helpful for general-purpose applications but problematic when strict schema adherence or specific logic is required.\n",
    "- Forcing an agent to comply with strict reasoning rules, intermediate steps, and output formats defeats its intended purpose and results in unnecessary complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage: What is (4 + 3 * 2) / (10 - 4)?\n",
      "AIMessage: \n",
      "ToolMessage: 7.0\n",
      "ToolMessage: 6.0\n",
      "ToolMessage: 6.0\n",
      "AIMessage: \n",
      "ToolMessage: 1.1666666666666667\n",
      "AIMessage: {\n",
      "  \"observation\": \"The division of (4 + 3 * 2) / (10 - 4) has been calculated.\",\n",
      "  \"thought\": \"The result is approximately 1.1667.\",\n",
      "  \"action\": null,\n",
      "  \"tool\": null,\n",
      " \"conclusion\": 1.1666666666666667\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create tools as LangChain Tool list:\n",
    "tools_list = [add, subtract, multiply, divide]\n",
    "\n",
    "agent = create_react_agent(model=llm, tools=tools, state_modifier=state_modifier, debug=False)\n",
    "query = \"What is (4 + 3 * 2) / (10 - 4)?\"\n",
    "response = agent.invoke({\"messages\": query})\n",
    "for message in response[\"messages\"]:\n",
    "    print(f\"{message.__class__.__name__}: {message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
